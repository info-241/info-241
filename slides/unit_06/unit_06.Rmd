---
title: "w241: Experiments and Causality"
subtitle: "Covariates and Regression"
author: "David Reiley, David Broockman, D. Alex Hughes"
institute: "UC Berkeley, School of Information"
date: "Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo=FALSE, message=FALSE, include=FALSE}
library(latex2exp)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(data.table)

berkeley_blue   <- '#003262'
california_gold <- '#FDB515'

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE, dpi = 300, fig.height = 3)


theme_set(theme_minimal())
```

# Returns to Schooling

## Reading: *Mastering Metrics* pages 209–211.
- Section 6.1 gives another example of regression and OVB in observational data.
- This regression:  
  - Includes a quadratic term.
  - The dependent variable is earnings. 
  - The main covariate is experience.
  - It includes both a linear experience term and an experience-squared term: 
    - This shows that earnings increase with experience but increase more slowly in later years.
    
---

class: inverse, cender, middle

# Work Experience as a Covariate

---

# Experience as a Covariate

## Goal: Estimate the "returns to schooling"

- How much does an additional year of schooling cause a person to earn? 
- Mincer includes *work experience* as a covariate: 
  - People with less schooling but much more work experience might earn more than people with more schooling but no work experience.

## Equation 6.2: 

$$
\begin{aligned}
    ln(Y_{i}) &= \alpha + 0.70 S_{i} + \epsilon_{i, short} \\ 
    ln(Y_{i}) &= \alpha + 0.107 S_{i} + 0.81 X_{i} - 0.0012 X_{i}^2 + \epsilon_{i, long}
\end{aligned}
$$ 
- The *short model* estimates an effect of $0.07$ on years of schooling
- The *long model* estimates an effect of $0.107$ on years of schooling? 

## Why are these estimates different? 

---

# Experience as a Covariate (cont'd) 

## Omitted Variables Bias 
- Omitted variable bias, *OVB*, leads us to underestimate the returns to schooling. **Why?**

## Obsesrvational Data 
- The two regressors -- *schooling* and *experience* can be correlated with each other!
  1. Schooling and work experience are negatively correlated with one another
  2. Schooling and experience are positively correlated with earnings. 
- If estimate the short model (we omit experience), its effect is measured as part of the schooling estimate
  - People with more schooling have less experience (negatively correlated)
  - When we increase schooling, earnings don't increase as much as they would if we were holding experience constant as a covariate.
  - Hence, with OVB we measure the coefficient of interest to be 7% instead of 11%.
  
---

# Quadratic Specification: Equation 6.2

- The quadratic specification allows for flexibility in the fit: rather than a linear effect, it permits a changing effect at different levels of the covariate
- In this case, we estimate that the benefits of experience to accrue at a declining rate. 

```{r make plots, echo = FALSE} 
d <- data.frame(x = seq(from=0, to=40, length.out = 200))
d$y_up   <- 1.28 + 0.081 * d$x - 0.0012 * (d$x)^2
d$y_down <- 1.28 + 0.081 * d$x + 0.0012 * (d$x)^2

plot_up <- d %>% 
  ggplot() + 
  aes(x = x, y = y_up) + 
  geom_line(color = berkeley_blue) + 
  labs(
    x = 'Years Experience, X', 
    y = 'Earnings', 
    title = TeX(r'($Y_{i} = 1.28 + 0.081X_{i} - 0.0012 X_{i}^{2}$)'))
plot_down <- d %>% 
  ggplot() + 
  aes(x = x, y = y_down) + 
  geom_line(color = california_gold) + 
  labs(
    x = 'Years Experience, X', 
    y = 'Earnings', 
    title = TeX(r'($Y_{i} = 1.28 + 0.081X_{i} + 0.0012 X_{i}^{2}$)'))

plot_up | plot_down

```

---

# Reading 

## Read: *Mastering Metrics* 211 - 214 

- Please read from the bottom of page 211 to the middle of page 214.

---

class: inverse, center, middle 

# Omitted Variable Bias and Attenuation Bias

---

class: center, middle 

## Is control for experience sufficient for **ceteris to be paribus**?

---

class: center, middle 
## Is control for experience sufficient for **all else to be equal**?

---

# Ability as an Omitted Variable 
- Griliches expected Mincer's estimates to be overstated
   - Omitted ability from the regression! 
   - Years of schooling are positively correlated with ability.
- When Griliches included IQ as a covariate, the estimated returns to schooling fell!
   - From 6.8% per year of schooling to 5.9% per year of schooling. 
   - Without IQ, OVB caused an overestimate of returns to schooling.

## All good? 
- After controlling for IQ as a measure of ability, Have we now controlled for everything that might cause biased estimates?
- **No!**. 
- There are more kinds of ability than IQ: emotional intelligence, curiosity, and many more. 
- We still have omitted variables that are likely to be correlated with years of schooling.

---

# Ability as an Omitted Variable (cont.)

- How do we know when we've got the *right* set of covariates so that we've got an unbiased estimate? 
  - **We can't know!**
- We would need an experiment that randomly sends some students to more years of school than others.
- Then every possible omitted variable would be uncorrelated with years of schooling, eliminating OVB.

---

# Attenuation Bias

## Angrist and Pischke 
  - Imagine that we don't always correctly measure the treatment variable (years of schooling).
  - With measurement error in the X variable, the resulting coefficient is biased toward zero.

## Effects of Online Advertising
  - Matched *Yahoo!* users to retail purchases using names and e-mail addresses. 
  - *Suppose the matching procedure allowed for nonexact matches.*
    - I might have some purchasers who I thought were in the control group, but who were really in the treatment group
    - **As a result** Some of the advertising effects would appear in the control group rather than in the treatment group, 
    - The effects would look smaller than actual.
    
---

# Reading 

## Optional Reading: *Mastering Metrics* pages 240-241
- This is the appendix to Chapter 6 and covers more about attenuation bias

## Required Reading: *Mastering Metrics* pages 214-217
- Next, read *Mastering Metrics*  pages 214–217 on bad controls. 
- Bad controls are a type of covariate we do not want in our regressions when we analyze experiments

---

class: inverse, center, middle

# Bad Controls 

---

class: center, middle

# Bad Controls 

## When you analyze experimental results, do not include other outcome variables as covariates on the right-hand side of the regression.

---

# Example: Random Assignment to College 

![table_6_1](./figures/table_6_1.png)
- True average treatment effect (ATE) is $500.

---

# Example: Random Assignment to College 

- Reminder, true ATE is $500

$$Y_{i} = \alpha + \beta E_{i} + \gamma W_{i} + \epsilon_{i}$$

- A regression on both a *college education* dummy, $E_{i}$, and a
*white-collar occupation* dummy, $W_{i}$: 
  - Yields a coefficient $\beta = 0$
  - Will mistakenly indicate that the return to college education is $0.
- Happens because: 
  - Only the **most**  talented non-college-educated workers will take white-collar jobs 
  - Only the **least** talented college-educated workers will take blue-collar jobs.  
  
---

# More About Bad Controls 

- We generally want to know the total effect of schooling on earnings. 
  - Schooling helps you become a data scientist, 
  - More educated data scientists earn more than less educated data scientists.
- Including the occupation covariate is therefore a bad idea: *It picks up only the latter kind of variation.*

![table_6_1](./figures/table_6_1.png)

---

# Example: eBay Reputation

## Does having a higher eBay reputation causes the seller to earn more revenue on eBay? 

- Two eBay seller accounts: 
 - One account with a low reputation 
 - One account with a high reputation
- Measures: 
 - Outcome, $Y$ = auction price
 - Treatment, $D$ = seller reputation 
 - Covariate, $X$ = number of bids

## (Bad Controls) Estimating Equation

$$Y = \beta_{0} + \beta_{1} D_{i} + \beta_{2} X_{i} + \epsilon_{i}$$

- Number of bids is a bad control. 

---

class: center, middle
## General principle: 
It is a bad idea to include posttreatment outcomes as covariates.

---

class: inverse, center, middle 

# Big Picture in Estimating Causal Effects

---

# Fundamentally Unanswerable Questions 

Some research questions are poorly posed

> "What is the effect on earnings of being born in Africa instead of North America?"

- What experiment could possibly answer this question?

---

# Questions to Ask 

1. What is the causal relationship of interest? 
2. What is the *ideal* experiment to measure this?
  - Even if you're doing observational research, *ask this question*! 
  - If your question seemed, FUQ'd, how should you refine your question? 
3. What is your *identification strategy*? 
  - Where does variation come from? 
  - Why is this variation independent of potential outcomes? 
4. How are you computing your confidence intervals? 

---

# Reading 

## Reading: Read *Mastering Metrics*, Chapter 2 Appendix, pages 95-97

---

class: inverse, center, middle 

# Robust Standard Errors and Confidence Intervals in Regression

---

class: center, middle

```{r, echo = FALSE}
d <- data.frame( 
  x_large_variation = runif(
    n = 50, min = 0, max = 10))
d$x_small_variation <- (d$x_large_variation+20)/5
d$y_large_variation <- 1 + d$x_large_variation + runif(n = 50, min = -3, max = 3)
d$y_small_variation <- 1 + d$x_large_variation + runif(n = 50, min = -0.5, max = 0.5)
d$y_small_x_variation <- 1 + d$x_small_variation + runif(n = 50, min = -3, max = 3) 

large_variance_in_y <- d %>% 
  ggplot() + 
  aes(x = x_large_variation, y = y_large_variation) + 
  geom_point(color = berkeley_blue) + 
  coord_cartesian(xlim = c(0,10), ylim = c(-2, 12), ) + 
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL) + 
  labs(
    title = 'Large Variance in Y', 
    x = '', y = ''
  )
small_variance_in_y <- d %>% 
  ggplot() + 
  aes(x = x_large_variation, y = y_small_variation) + 
  geom_point(color = california_gold) + 
  coord_cartesian(xlim = c(0,10), ylim = c(-2, 12)) +
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL) +   
  labs(
    title = 'Small Variance in Y', 
    x = '', y = ''
  )
small_variance_in_x <- d %>% 
  ggplot() + 
  aes(x = x_small_variation, y = y_small_x_variation) + 
  geom_point(color = '#B9D3B6') + 
  coord_cartesian(xlim = c(0,10), ylim = c(-2, 12)) + 
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL) +   
  labs(
    title = 'Small Variance in X', 
    x = '', y = ''
  )

large_variance_in_y | small_variance_in_y | small_variance_in_x
```

---

# Standard Errors and Confidence Intervals 

## How reliably have we estimated our slope coefficient? 

- Where does any *noise* come from? 

## Rules of Thumb about Standard Errors 

- SEs are larger when variance in $Y$ is larger 
- SEs are smaller when variance in $X$ is larger 

---

```{r, echo = FALSE}
large_variance_in_y | small_variance_in_y | small_variance_in_x
```

---

# Standard Errors in Regression Output 

- Treatment effect (with 95 percent confidence interval) $\approx$ slope coefficient ± 2 standard errors
- OLS standard errors assume each observation's idiosyncratic component,  $\epsilon$ is iid (independent and identically distributed)
- Independence is sensible in a randomized experiment

## Why should we expect all points to have the same variance?

---

# Heteroskedasticity 

## Heteroskedastic & Homoskedastic Errors
- **Hetero-skedastic**: Different observations have different error variances
- **Homo-skedastic**: Different observations have the same error variances
  - *We don't actually write it with the hyphen, but it makes it more clear to read*

## OLS Defaults
- Homoskedasticity is the default assumption under which OLS standard errors are usually computed
- Vertical error variance causes uncertainty about line's true slope

---

# Distributions of Data 

```{r, echo = FALSE}
d <- data.frame( 
  x = seq(from = 0, to = 10, length.out = 100))

d$y_homoskedastic   <- 1 + d$x + rnorm(n = 100, mean = 0, sd = 1)
d$y_heteroskedastic <- 1 + d$x + rnorm(n = 100, mean = 0, sd = seq(from = 0.1, to = 3, length.out = 100))
d$y_heteroskedastic_2 <- 1 + d$x + rnorm(
  n = 100, mean = 0, sd = c(
    seq(from=0.1, to=3, length.out=50), 
    seq(from=3, to=0.1, length.out=50)
    ))

plot_homoskedastic <- d %>% 
  ggplot() + 
  aes(x = x, y = y_homoskedastic) + 
  geom_point(color = berkeley_blue) + 
  coord_cartesian(xlim = c(0, 10), ylim = c(0,15)) + 
  scale_x_continuous(labels = NULL) + 
  # scale_y_continuous(labels = NULL) + 
  labs(
    title = 'Homoskedastic Data',
    x = '', y = ''
  )

plot_heteroskedastic <- d %>% 
    ggplot() + 
  aes(x = x, y = y_heteroskedastic) + 
  geom_point(color = california_gold) + 
  coord_cartesian(xlim = c(0, 10), ylim = c(0,15)) + 
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL) +
  labs(
    title = 'Heteroskedastic Data',
    x = '', y = ''
  )

lucky_heteroskedastic <- d %>% 
  ggplot() + 
  aes(x = x, y = y_heteroskedastic_2) + 
  geom_point(color = '#B9D3B6') + 
  coord_cartesian(xlim = c(0, 10), ylim = c(0,15)) + 
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL) +
  labs(
    title = 'Heteroskedastic Luck',
    x = '', y = ''
  )


plot_homoskedastic | plot_heteroskedastic | lucky_heteroskedastic
```

- *Fanned Out* data means many lines could be fit, depending on the sample 
- More accurate plot with accuracy *distant* from grand mean: endpoints anchor slope.
- Leverage: Data points nearer ends of regression line influence slope more.

---

# Robust Standard Errors 
## Robust Standard Errors 
- Estimate accurate confidence intervals, even when error variance varies with $X$
- Also known as *heteroskedasticity-robust* standard errors or *Eicher-White*, or *Huber-White* standard errors
- Do not require knowledge of: 
  1. shape of heteroskedasticity; or, 
  2. Which X-variables correlate with variance

## Optional Reading 
- Page 45 of *Mostly Harmless Econometrics* (the PhD version of *Mastering Metrics* describes technical details and matrix algebra). 
- Shows that extreme values of $X$ have more leverage on slope coefficient, $\beta$.

---

# Accounting for Leverage 

- When estimating variance of $\hat{\beta}$: 
  - Take weighted sum of squared residuals (i.e., squared vertical deviations from regression line)
  - Divide by total variance in $X$
  - Weights in weighted sum correspond to leverage of each observation
  - Squared residuals, $\hat{\epsilon}^2$, weighted by squared horizontal deviations from the mean
  
---

# Tennessee STAR Experiment 

- Randomized at classroom level
- Each classroom's students had same teachers, similar backgrounds/experiences
- More similarity within each classroom than between classrooms
- Changing 20 similar students from control group to treatment group moves potential Y for all 20 at once
  - Result: more variance than if 20 randomly chosen people had been moved
- Clustered Standard Errorss account for lack of independence, asd RSEs account for heteroskedasticity
   - In Tennessee STAR experiment, CSEs are about three times larger than OLS standard errors
  - Further technical details, see MHE 8.2
  
---

# Takeaways
## Takewaways about Regression Uncertainty

- Best practice to always use *robust standard errors*
  - Do not assume homoskedastic errors 
  - Pay only a small penalty if we're incorrect (i.e. the data is *actually* homoskedastic) 
- If treatment assignmet is clustered, **must** use clustered standard errors to avoid unintentionally overstating precision of estimates

---

class: inverse, center, middle 

# Multifactor Experiments 

---

# Multifactor Experiments 

## Reading: *Field Experiments* section 9.3.3
- Presents multi-factor experiments -- experiments that have more than a single treatment 

## Estimating effects in a multi-factor experiment 
- Estimate regressions with interaction terms to estimate how much more *one* treatment matters when the *other* treatment is turned on

---

class: inverse, center, middle 

# Example: The Visible Hand 

---

# Doleac and Stein (2013) 
## Example: The Visible Hand

- Conducted an experiment on Craigslist (this is now **very** hard to do successfully) to assess race- and class-based discrimination among consumers
  - Ran ads to sell iPods in different local markets
  - Measured average offer
  - Measured average number of offers

## Treatment Dimensions 

1. Race and Class 
  - White hand
  - White hand with tattoo 
  - Black hand 
2. Ad quality 
  - Grammar or spelling errors
  - No grammar or spelling errors 
3. Asking price: $90, $110, or $130

---

class: center, middle 

![visible hand](./figures/visible_hand.png)

---

class: center, middle 

![offer text](./figures/offer_text.png)

---

# Doleac and Stein (2013) 
## Results 

- Both race and tattoo had much more treatment effect than ad quality.

## Reading: Field Experiments Section 9.3.3 
- Please read this section with the following question in mind
> "Do Black sellers hurt themselves more with bad grammar than White sellers do?"

- You can ignore the last two paragraphs containing subtle points.

---

class: inverse, center, middle 

# Multifactor Designs 

---

# 2x2 Designs 
## Four treatments summarized with a 2x2 table

- In this experiment, there were four different treatments
  1. *Colin* with Good Grammar 
  2. *Colin* with Bad Grammar 
  3. *Jose* with Good Grammar 
  4. *Jose* with Bad Grammar

|                | Colin | Jose | 
|----------------|-------|------|
| *Good Grammar* | 52%   | 37%  | 
| *Bad Grammar*  | 29%   | 34%  | 

---

# Craigslist Experiment Design 

## 3 x 2 x 3 
  - 3 photo conditions
  - 2 grammar conditions
  - 3 price conditions

## 6 ad texts for 2 grammar conditions
  - But only 2 grammar conditions were the treatments of interest; we aggregate other variants by grammar condition
  
---

# Reading 

## Reading: *Field Experiments* Section 9.4 through page 304
- The first two pages present using regression to estimate the results of a multi-factor experiment 
- In Equation 9.10, the expression between the two equals signs is compact notation for the following definition: 
  - $Y = Y_{i}(0), \text{ if } d_{i} = 0$
  - $Y = Y_{i}(1), \text{ if } d_{i} = 1$

---

class: inverse, center, middle 

# Regression Analysis of Multifactor Experiments 

---

# Regression Specification 

## Equation 9.11 
Define the following symbols: 
  - $NH\_GG$: Non-Hispanic, Good Grammar
  - $H\_GG$: Hispanic, Good Grammar 
  - $NH\_BG$: Non-Hispanic, Bad Grammar 
  - $H\_BG$: Hispanic, Bad Grammar

$$Y_{i} = \beta_{1}NH\_GG + \beta_{2}H\_GG + \beta_{3}NH\_BG + \beta_{4}H\_BG + u_{i}$$

---

# Regression Specification (cont'd) 

## Equation 9.12
Define the following symbols 
  - $J$: Takes value 1 if letter sent from "Jose"; 0 if letter sent from "Colin" 
  - $G$: Takes value 1 if letter has "Good Grammar"; 0 if "Bad Grammar"

$$Y_{i} = \alpha + \beta (J_{i}) + \gamma (G_{i}) + \delta (J_{i}\times G_{i}) + u_{i}$$

- Equation 9.12 and 9.11 are equivalent
- *Related* estimated parameters -- see the equations that begin on page 306

---

# Estimates from Equation 9.11 

$$Y_{i} = \beta_{1}NH\_GG + \beta_{2}H\_GG + \beta_{3}NH\_BG + \beta_{4}H\_BG + u_{i}$$

- Equation 9.11 directly estimates averages within each treatment condition
- Coefficients $\beta_{1}, \dots, \beta_{4}$ estimate numbers in table 

|                | Colin | Jose | 
|----------------|-------|------|
| *Good Grammar* | 52%   | 37%  | 
| *Bad Grammar*  | 29%   | 34%  | 

---

# Data Structure 9.11 
## Estimating Equation 

$$Y_{i} = \beta_{1}NH\_GG + \beta_{2}H\_GG + \beta_{3}NH\_BG + \beta_{4}H\_BG + u_{i}$$

## Data Structure

|ID        | Y        | NH_GG    | H_GG     | NH_BG    | H_BG     | 
|----------|----------|----------|----------|----------|----------|
| 1        | Yes      | 1        | 0        | 0        | 0        | 
| 2        | Yes      | 0        | 0        | 1        | 0        | 
| 3        | No       | 0        | 1        | 0        | 0        | 
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | 
| 400      | No       | 0        | 1        | 0        | 0        |  

---

# Estimating Coefficients for 9.12 

$$Y_{i} = \alpha + \beta (J_{i}) + \gamma (G_{i}) + \delta (J_{i}\times G_{i}) + u_{i}$$


## This specification measures *differences* between cells.
- $\alpha$ estimates the average response in the omitted category (Colin, Good Grammar).
- $\beta$ estimates the effect of ethnicity *when there are no grammar errors*, `G=0`. 
- $\gamma$ estimates the effect of grammar, *when the ethnicity signal is Colin*, `J=0`.
- $\delta$, the interaction coefficient, estimates how much more the grammar errors matter for Jose than for Colin.

---

# Interaction Coefficient in Equation 9.12

$$Y_{i} = \alpha + \beta (J_{i}) + \gamma (G_{i}) + \delta (J_{i}\times G_{i}) + u_{i}$$

## Suppose that `G=1`, the sender uses Good Grammar
- What happens when `J=0` $\rightarrow$ `J=1`? 
- Regression can make this analysis simpler
  - You can obtain results all at one time.
  - Results can be easier to interpret when coefficients are measuring differences instead of levels.
  
---

# Estimated Coefficients
## Estimtes are provided in Equation 9.16

$$\widehat{Y} = 0.52 - 0.15(J_{i}) - 0.23 (G_{i}) + 0.20 (J_{i}G_{i})$$
## Data Structure

|ID        | Y        | J        | G        | 
|----------|----------|----------|----------|
| 1        | Yes      | 0        | 0        |
| 2        | Yes      | 1        | 0        |
| 3        | No       | 1        | 1        |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | 
| 400      | No       | 1        | 1        |

---

# Interpreting Estimates in 9.16
## Estimating Equation
$$\widehat{Y} = 0.52 - 0.15(J_{i}) - 0.23 (G_{i}) + 0.20 (J_{i}G_{i})$$

## Summary Table
|                | Colin | Jose | 
|----------------|-------|------|
| *Good Grammar* | 52%   | 37%  | 
| *Bad Grammar*  | 29%   | 34%  | 

## Interpretation 
- Regression coefficients found by subtracting numbers in the data table
- $0.52$ is the fraction of letters sent that received a response *baseline condition*
- $0.15$ the difference between `J=0` $\rightarrow$ `J=1`, when `G=0`. 
- $0.52 - 0.37 = 0.15$

---

# Interpreting Interaction Term
## Estimating Equation
$$\widehat{Y} = 0.52 - 0.15(J_{i}) - 0.23 (G_{i}) + 0.20 (J_{i}G_{i})$$

## Summary Table
|                | Colin | Jose | 
|----------------|-------|------|
| *Good Grammar* | 52%   | 37%  | 
| *Bad Grammar*  | 29%   | 34%  | 

## Interpreting Interation Term

$$
\begin{aligned}
  0.34 - 0.37 &= -0.03 \\ 
  0.29 - 0.52 &= -0.23 \\ 
  -0.03 - (-0.23) &= \mathbf{0.20}
\end{aligned}
$$

---

# Interpreting Interaction Term (cont'd) 
## Estimating Equation
$$\widehat{Y} = 0.52 - 0.15(J_{i}) - 0.23 (G_{i}) + 0.20 (J_{i}G_{i})$$

## Summary Table
|                | Colin | Jose | 
|----------------|-------|------|
| *Good Grammar* | 52%   | 37%  | 
| *Bad Grammar*  | 29%   | 34%  | 

## Interpretation 
- How much more does bad grammar, `G=1` matter with `J=1` vs. `J=0`?

---

# Presenting Regression Results 

Regression output automatically includes standard errors, for easy hypothesis testing.

## Do grammar errors have less impact when letters are received from Jose vs. Colin?
- Perform a Wald-test on the regression coefficient: $\frac{\delta}{SE(\delta)}$
- The authors performed an F-test, which is unnecessary in this case with a single interaction term

## Regression for more complex variable expression
- Regression gracefully handles non-binary categorical variables (where an F-test would be required)
  - One example is the amount of someone's schooling.
  
---

# What to Remember

## Bad Controls 
- Do not include post-treatment covariates in the regression
- This can be especially tempting when you have a robust set of user-data

## Standard Errors 
- Robust standard errors are *always* a good idea
- They are of little cost if they are unnecessary, and are required with *heteroskedastic* errors
- Use clustered standard errors in regressions that have clustered treatment assignment; failing to do so will produce estimates that are incorrectly precise 

## Multifactor Experiments 
- We can expand the complexity of treatment we assign by *crossing* treatment features
- Regression with dummy variables quickly, efficiently, and unbiasedly estimates the effects of these experiments.

---

class: inverse, center, middle

# Lessons to Remember 

---

# Lessons to Remember 

## Lessons to Remember

- Quadratic terms in regression allow us to assess whether effects are increasing or decreasing in a continuous covariate.
- **Attenuation bias**: When X is mismeasured, our estimated treatment effect will be biased toward zero.
- **Bad controls**: In experiments, don't use covariates that could have been affected by the treatment.
- **Robust standard errors** are always a good idea. We don't have to know how the variance of the error term changes with covariates; the formula takes care of this for us.
- **Clustered standard errors** are necessary when we have clustered treatment assignment (see FE 3.6.2).
- **Multifactor experiments** have more than one treatment at a time.
- We can use regression with dummy variables to quickly and conveniently summarize the treatment effects and interaction effects.