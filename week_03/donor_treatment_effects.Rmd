---
title: "Week 03 Live Session"
output: html_notebook
---

# Donor Treatment effect? 

Let's look at the box that's reported in _Field Experiments_ box 3.7. This is a short table of "donations" to a political campaign, where some people have received one treatment message about donation, while other have received another message. 

```{r}
library(data.table)
library(magrittr)

d <- fread('http://www.dropbox.com/s/wfao23f30ll9jo1/donation.csv?raw=1')

## d <- data.table(
##   Z = rep(1:0, each = 10),
##   Y = c(500, 100, 100, 50, 25, 25, 0, 0, 0, 0,
##         25,  20,  15,  15, 10, 5,  5, 0, 0, )
## )

head(d)
hist(d$Y, breaks=20)

# Normal data
# nd <- data.table(Y = c(rnorm(1000, 0, 1), rnorm(1000, 3, 1)),
#                  Z = c(rep("T", 1000), rep("C", 1000)))
# nd
```
Here are the column definitions: 

- `Y`: The amount of donations that were made by an individual 
- `Z`: Whether that individual was in the new contact language treatment, or the old contact language treatment. 

# Conduct the following work

1. Assess whether the treatment _has an effect_. If so, what is that effect, on average? We'll give you the general structure of a solution, but you'll have to fill in the specifics. 

```{r}
ate <- d[ , .(group_mean = mean(Y)), keyby = .(sample(Z))][ , .('ate' = diff(group_mean))]
ate
```

2. Assess, using randomization inference, whether this effect is a _surprising_ effect, or instead if this effect could just have arisen from randomization error. 

In doing so, **do not** simply copy code from the async, or from another notebook; actually think about the algorithm that you want to perform, and write it from scratch. 

What are the steps to any Randomization Inference Task? 

1. Conduct the experiment, using random assignment to produce two unbiased samples of the desired test statistic. 
2. Compute the desired test statistic (here the ATE) on this experimental data.
3. Suppose that the sharp null hypothesis is true, thereby solving the *missing data* problem that is at the heart of the fundamental problem of causal inference, and permitting the complete observation of all potential outcomes under the supposition of the sharp null were true. 
4. Repeatedly sample the treatment assignment vector and under each assignment, for each sample, compute the test statistic in the same manner as in step 1. 
5. Compare the measured treatment effect to the distribution of treatment effects under the sharp null supposition and directly draw a p-value. 

You can do it! There are ways that involve loops that are pretty easy to think about. We have even shown them to you in previous worksheets. **But** rather than navigating back to the last worksheet, instead try to write this yourself following the steps. 

(**Note**: You're going to generate a surprising distribution, this doesn't mean that your algorithm isn't working. Why might it look this way? We'll talk about this in live session.)

```{r}

# Function
ri <- function(simulations = 100) {
  
  # Bucket for test statistic (ATE)
  res <- NA   
  
  # For every "possible randomization"
  for(sim in 1:simulations) { 
    # Fill the bucket with ATEs AFTER shuffle/permute/randomize/Z treatment vector, Z
    res[sim] <- d[ , .(group_mean = mean(Y)), 
                   keyby = .(sample(Z))][ , diff(group_mean)]
  }
  # Return the bag of test statistics
  return(res)
}

# Run it
sharp_null_dist <-ri(100000)


```

# Questions for understanding 

1. Characterize the distribution of the sharp-null distribution of treatment effects. Talk about what, if anything, is notable about it, and what components of the data might be leading to any patterns that you note.

```{r}
# sampling distribution
hist(sharp_null_dist, breaks=100)
abline(v=ate, col ="red")
```

2. How many of the randomization inference loops are larger than the treatment effect that you calculated? How would you use this statement to construct a one-sided test, and an associated p-value? 

```{r}
# One sided p-value
sum(sharp_null_dist>ate[[1]])/length(sharp_null_dist)
```


3. How many of the randomization inference loops are _more extreme_ (:metal:) than the treatment effect that you calculated? How would you use this statement to construct a two-sided test, and an associated p-value? 

```{r}
# Two sided p-value
sum(abs(sharp_null_dist)>ate[[1]])/length(sharp_null_dist)
```

4. Compare the two-sided p-value against the p-value that you generate from a two-tailed t-test. If these p-values are the same, would this be a positive or a negative characteristic of randomization inference? If these p-values are different, why would they be different? Don't go looking all over hill-and-dale for the call for a t-test, it is at `t.test`. 


5. Which of the two of these inferential methods do you prefer, randomization inference or a t-test, and why? Ease of use is not an acceptable answer. 

```{r}
t.test(Y~Z, data=d)
```